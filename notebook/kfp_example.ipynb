{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_APPLICATION_CREDENTIALS is set to: ../fleet-anagram-244304-449e515c8d17.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the GOOGLE_APPLICATION_CREDENTIALS environment variable\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../fleet-anagram-244304-449e515c8d17.json\"\n",
    "\n",
    "# Verify the environment variable is set\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"GOOGLE_APPLICATION_CREDENTIALS is not set\"\n",
    "print(\"GOOGLE_APPLICATION_CREDENTIALS is set to:\", os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl, compiler\n",
    "from kfp.dsl import (\n",
    "    component,\n",
    "    Dataset,\n",
    "    Input,\n",
    "    Output,\n",
    "    InputPath,\n",
    "    OutputPath,\n",
    "    Metrics\n",
    ")\n",
    "from google.cloud.aiplatform import pipeline_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.cache/pypoetry/virtualenvs/rental-prediction-jRa7Y522-py3.10/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:126: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.9' to 'python:3.10' on Oct 1, 2025. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.10.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@dsl.component(packages_to_install=[\"pandas\", \"pandera\", \"loguru\", \"gcsfs\"])\n",
    "def validate_input_ds(\n",
    "    file_path: str,\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    import datetime\n",
    "    import pandas as pd\n",
    "    import pandera as pa\n",
    "    from loguru import logger\n",
    "    from pandera.typing import Series\n",
    "\n",
    "    # Initialize logging\n",
    "    logger.add(\"validate_input_ds.log\")\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    class DfSchema(pa.DataFrameModel):\n",
    "        address: Series[str] = pa.Field(nullable=False)\n",
    "        area: Series[float] = pa.Field(gt=0, nullable=False)\n",
    "        constraction_year: Series[int] = pa.Field(gt=0)\n",
    "        rooms: Series[int] = pa.Field(gt=0, nullable=False)\n",
    "        bedrooms: Series[int] = pa.Field(gt=0, nullable=False)\n",
    "        bathrooms: Series[int] = pa.Field(gt=0, nullable=False)\n",
    "        balcony: Series[str] = pa.Field(isin=[\"yes\", \"no\"], nullable=False)\n",
    "        storage: Series[str] = pa.Field(isin=[\"yes\", \"no\"], nullable=False)\n",
    "        parking: Series[str] = pa.Field(isin=[\"yes\", \"no\"], nullable=False)\n",
    "        furnished: Series[str] = pa.Field(isin=[\"yes\", \"no\"], nullable=False)\n",
    "        garage: Series[str] = pa.Field(isin=[\"yes\", \"no\"], nullable=False)\n",
    "        garden: Series[str] = pa.Field(nullable=False)\n",
    "        energy: Series[str] = pa.Field(nullable=True)\n",
    "        facilities: Series[str] = pa.Field(nullable=True)\n",
    "        zip: Series[str] = pa.Field(nullable=False)\n",
    "        neighborhood: Series[str] = pa.Field(nullable=False)\n",
    "        rent: Series[int] = pa.Field(gt=0, nullable=False)\n",
    "\n",
    "        @pa.check(\"garden\", error=\"Must start with 'Present' or 'Not Present'\")\n",
    "        def check_garden(cls, series: Series[str]) -> Series[bool]:\n",
    "            return series.str.startswith((\"Present\", \"Not present\"))\n",
    "\n",
    "        @pa.check(\"energy\", error=\"Must start with A-G\")\n",
    "        def check_energy(cls, series: Series[str]) -> Series[bool]:\n",
    "            return series.isna() | series.str.startswith(tuple(\"ABCDEFG\"))\n",
    "\n",
    "        @pa.check(\"zip\", error=\"Must start with numeric\")\n",
    "        def check_zip(cls, series: Series[str]) -> Series[bool]:\n",
    "            return series.str.match(r\"^\\d\")\n",
    "\n",
    "        @pa.check(\"constraction_year\", error=\"Year must be between 1800 and next year\")\n",
    "        def check_contraction_year(cls, series: Series[int]) -> Series[bool]:\n",
    "            current_year = datetime.datetime.now().year\n",
    "            return (series >= 1000) & (series <= current_year + 1)\n",
    "\n",
    "        @pa.check(\"rent\", error=\"Rent must match property features\")\n",
    "        def check_rent_plausibility(cls, series: Series[int]) -> Series[bool]:\n",
    "            return series > 0\n",
    "\n",
    "    try:\n",
    "        validated_df = DfSchema.validate(df)\n",
    "        logger.info(\"Data validation passed\")\n",
    "    except pa.errors.SchemaError as e:\n",
    "        logger.error(f\"Data validation failed: {e}\")\n",
    "        raise\n",
    "\n",
    "    validated_df.to_csv(dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(packages_to_install=[\"pandas\", \"gcsfs\"])\n",
    "def process_validated_data(\n",
    "    dataset: Input[Dataset],\n",
    "    dataset_out: Output[Metrics]\n",
    "):  \n",
    "    import re\n",
    "    import pandas as pd\n",
    "\n",
    "    # Read the validated data\n",
    "    with open(dataset.path, \"r\") as train_data:\n",
    "        df = pd.read_csv(train_data)\n",
    "    \n",
    "    processed_df= pd.get_dummies(df, \n",
    "                              columns = ['balcony',\n",
    "                                         'parking', \n",
    "                                         'furnished', \n",
    "                                         'garage', \n",
    "                                         'storage'], \n",
    "                              drop_first=True)\n",
    "\n",
    "    processed_df['garden'] = processed_df['garden'].apply(\n",
    "        lambda x: 0 if x == 'Not present' else int(re.findall(r'\\d+', x)[0])\n",
    "    )\n",
    "    \n",
    "    # Save the processed data\n",
    "    processed_df.to_csv(dataset_out.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    pipeline_root=\"gs://hy-storage-bucket\",\n",
    "    name=\"test-pipeline\",\n",
    ")\n",
    "def pipeline(project: str = \"fleet-anagram-244304\", region: str = \"us-central1\"):\n",
    "    file_name = \"gs://hy-storage-bucket/rent_apartments.csv\"\n",
    "    input_validation_task = validate_input_ds(file_path=file_name)\n",
    "\n",
    "    processed_data_task = process_validated_data(\n",
    "        dataset=input_validation_task.outputs[\"dataset\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/test-pipeline-20250202144556?project=465835940757\n",
      "PipelineJob projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/465835940757/locations/us-central1/pipelineJobs/test-pipeline-20250202144556\n"
     ]
    }
   ],
   "source": [
    "from kfp.v2 import compiler, dsl\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='test-pipeline.json'\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "    display_name=\"simple-kubeflow-pipeline\",\n",
    "    template_path=\"test-pipeline.json\",\n",
    "    enable_caching=False,\n",
    "    location=\"us-central1\",\n",
    ")\n",
    "\n",
    "start_pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rental-prediction",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
